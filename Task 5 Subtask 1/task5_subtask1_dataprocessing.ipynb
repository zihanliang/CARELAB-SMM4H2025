{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Download necessary nltk resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zihanliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zihanliang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/zihanliang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMM4H-2025-Task5-Train_subtask1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing complete. Output saved to: preprocessed_SMM4H-2025-Task5-Train_subtask1.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data Preprocessing and Data Augmentation (Hierarchical Processing Version)\n",
    "\n",
    "This code is based on the SMM4H-2025-Task5-Train_subtask1.csv file and performs the following steps on the original news text:\n",
    "1. Clean the text: remove HTML tags, URLs, redundant spaces, etc.;\n",
    "2. Manual segmentation (Chunking): use sentence boundaries to segment the text, ensuring that the token count of each segment does not exceed 512.\n",
    "   If a sentence itself is longer than 512 tokens, it will be further split to preserve complete information.\n",
    "3. Data augmentation: perform synonym replacement on the text, avoiding replacement of domain keywords (e.g., FDA, Listeria, etc.);\n",
    "4. Finally, save the cleaned text, segmented text list, and augmented text into a new CSV file.\n",
    "\n",
    "Please ensure that the SMM4H-2025-Task5-Train_subtask1.csv file is located in the same directory as the Notebook, or modify the file paths in the code.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the text:\n",
    "      - Remove HTML tags\n",
    "      - Replace URLs with \"URL\"\n",
    "      - Remove extra spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text, flags=re.MULTILINE)  # Replace URLs\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "def split_long_sentence(sentence: str, tokenizer, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Split a sentence into smaller chunks if its token count exceeds max_length.\n",
    "    The sentence is tokenized, then split into chunks, and each chunk is converted back to a string.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i+max_length]\n",
    "        # Convert tokens back to string; this may not be perfectly formatted but preserves all information.\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def segment_text(text: str, tokenizer, max_length: int = 512) -> list:\n",
    "    \"\"\"\n",
    "    Segment the text using sentence boundaries (Chunking) for hierarchical processing.\n",
    "    If the total token count of the text does not exceed max_length, return the original text directly;\n",
    "    Otherwise, split the text by sentences. For each sentence, if its token count exceeds max_length,\n",
    "    further split it into smaller chunks. Then, gradually combine chunks/sentences ensuring that each segment's token count does not exceed max_length.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    # Split text into sentences using nltk\n",
    "    sentences = sent_tokenize(text)\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence itself exceeds max_length tokens\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if len(sentence_tokens) > max_length:\n",
    "            # Further split the long sentence into smaller chunks\n",
    "            parts = split_long_sentence(sentence, tokenizer, max_length)\n",
    "        else:\n",
    "            parts = [sentence]\n",
    "        \n",
    "        # Process each part/chunk\n",
    "        for part in parts:\n",
    "            if current_segment:\n",
    "                combined = (current_segment + \" \" + part).strip()\n",
    "            else:\n",
    "                combined = part\n",
    "            if len(tokenizer.tokenize(combined)) <= max_length:\n",
    "                current_segment = combined\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(current_segment)\n",
    "                current_segment = part\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    return segments\n",
    "\n",
    "def get_synonyms(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Use nltk's WordNet to get a list of synonyms for the word,\n",
    "    filtering out candidates that are identical to the original word and replacing underscores with spaces.\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            candidate = lemma.name().replace(\"_\", \" \")\n",
    "            if candidate.lower() != word.lower():\n",
    "                synonyms.add(candidate)\n",
    "    return list(synonyms)\n",
    "\n",
    "def augment_text(text: str, replacement_prob: float = 0.1, skip_words: set = None) -> str:\n",
    "    \"\"\"\n",
    "    Perform data augmentation on the text:\n",
    "      - Replace each word with a synonym with a certain probability (replacement_prob)\n",
    "      - Protect domain keywords (e.g., FDA, Listeria, E. coli, etc.) from replacement\n",
    "    \"\"\"\n",
    "    if skip_words is None:\n",
    "        skip_words = {\"FDA\", \"Listeria\", \"E. coli\", \"Salmonella\", \"Maytag\", \"CDC\"}\n",
    "    \n",
    "    words = text.split()\n",
    "    augmented_words = []\n",
    "    for word in words:\n",
    "        # Only replace alphabetic words that are not in the protection list\n",
    "        if word.isalpha() and word not in skip_words and random.random() < replacement_prob:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                new_word = random.choice(synonyms)\n",
    "                augmented_words.append(new_word)\n",
    "            else:\n",
    "                augmented_words.append(word)\n",
    "        else:\n",
    "            augmented_words.append(word)\n",
    "    return \" \".join(augmented_words)\n",
    "\n",
    "def process_data(input_file: str,\n",
    "                 output_file: str,\n",
    "                 tokenizer_name: str = \"bert-base-uncased\",\n",
    "                 max_length: int = 512,\n",
    "                 augmentation_prob: float = 0.1):\n",
    "    \"\"\"\n",
    "    Overall data preprocessing and augmentation process:\n",
    "      1. Read CSV data (the file contains the fields: docid, text, Subtask1_Label)\n",
    "      2. Clean each article to generate cleaned_text\n",
    "      3. Generate segments (in list form) using manual segmentation based on max_length\n",
    "      4. Perform data augmentation on the cleaned text to generate augmented_text\n",
    "      5. Save the results into a new CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    segments_list = []\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        original_text = str(row[\"text\"])\n",
    "        # 1. Clean the text\n",
    "        cleaned = clean_text(original_text)\n",
    "        # 2. Manual segmentation (hierarchical processing): obtain a list of segments with token count not exceeding max_length\n",
    "        segments = segment_text(cleaned, tokenizer, max_length)\n",
    "        # 3. Data augmentation (synonym replacement)\n",
    "        augmented = augment_text(cleaned, replacement_prob=augmentation_prob)\n",
    "        \n",
    "        cleaned_texts.append(cleaned)\n",
    "        segments_list.append(segments)\n",
    "        augmented_texts.append(augmented)\n",
    "    \n",
    "    df[\"cleaned_text\"] = cleaned_texts\n",
    "    df[\"segments\"] = segments_list\n",
    "    df[\"augmented_text\"] = augmented_texts\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"[INFO] Preprocessing complete. Output saved to: {output_file}\")\n",
    "\n",
    "# ===============================================\n",
    "# Run the following code in Jupyter Notebook\n",
    "# ===============================================\n",
    "input_file = \"SMM4H-2025-Task5-Train_subtask1.csv\"  # Input file path\n",
    "output_file = \"preprocessed_SMM4H-2025-Task5-Train_subtask1.csv\"  # Output file path\n",
    "tokenizer_name = \"bert-base-uncased\"  # Change the model name if needed\n",
    "max_seq_length = 512  # Maximum token count for Transformer models\n",
    "augmentation_probability = 0.1  # Probability for synonym replacement\n",
    "\n",
    "process_data(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    max_length=max_seq_length,\n",
    "    augmentation_prob=augmentation_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMM4H-2025-Task5-Validation_subtask1.csv Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing complete. Output saved to: preprocessed_SMM4H-2025-Task5-Validation_subtask1.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data Preprocessing and Data Augmentation (Hierarchical Processing Version)\n",
    "\n",
    "This code is based on the SMM4H-2025-Task5-Validation_subtask1.csv file and performs the following steps on the original news text:\n",
    "1. Clean the text: remove HTML tags, URLs, redundant spaces, etc.;\n",
    "2. Manual segmentation (Chunking): use sentence boundaries to segment the text, ensuring that the token count of each segment does not exceed 512.\n",
    "   If a sentence itself is longer than 512 tokens, it will be further split to preserve complete information.\n",
    "3. Data augmentation: perform synonym replacement on the text, avoiding replacement of domain keywords (e.g., FDA, Listeria, etc.);\n",
    "4. Finally, save the cleaned text, segmented text list, and augmented text into a new CSV file.\n",
    "\n",
    "Please ensure that the SMM4H-2025-Task5-Validation_subtask1.csv file is located in the same directory as the Notebook, or modify the file paths in the code.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the text:\n",
    "      - Remove HTML tags\n",
    "      - Replace URLs with \"URL\"\n",
    "      - Remove extra spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text, flags=re.MULTILINE)  # Replace URLs\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "def split_long_sentence(sentence: str, tokenizer, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Split a sentence into smaller chunks if its token count exceeds max_length.\n",
    "    The sentence is tokenized, then split into chunks, and each chunk is converted back to a string.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i+max_length]\n",
    "        # Convert tokens back to string; this may not be perfectly formatted but preserves all information.\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def segment_text(text: str, tokenizer, max_length: int = 512) -> list:\n",
    "    \"\"\"\n",
    "    Segment the text using sentence boundaries (Chunking) for hierarchical processing.\n",
    "    If the total token count of the text does not exceed max_length, return the original text directly;\n",
    "    Otherwise, split the text by sentences. For each sentence, if its token count exceeds max_length,\n",
    "    further split it into smaller chunks. Then, gradually combine chunks/sentences ensuring that each segment's token count does not exceed max_length.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    # Split text into sentences using nltk\n",
    "    sentences = sent_tokenize(text)\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence itself exceeds max_length tokens\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if len(sentence_tokens) > max_length:\n",
    "            # Further split the long sentence into smaller chunks\n",
    "            parts = split_long_sentence(sentence, tokenizer, max_length)\n",
    "        else:\n",
    "            parts = [sentence]\n",
    "        \n",
    "        # Process each part/chunk\n",
    "        for part in parts:\n",
    "            if current_segment:\n",
    "                combined = (current_segment + \" \" + part).strip()\n",
    "            else:\n",
    "                combined = part\n",
    "            if len(tokenizer.tokenize(combined)) <= max_length:\n",
    "                current_segment = combined\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(current_segment)\n",
    "                current_segment = part\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    return segments\n",
    "\n",
    "def get_synonyms(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Use nltk's WordNet to get a list of synonyms for the word,\n",
    "    filtering out candidates that are identical to the original word and replacing underscores with spaces.\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            candidate = lemma.name().replace(\"_\", \" \")\n",
    "            if candidate.lower() != word.lower():\n",
    "                synonyms.add(candidate)\n",
    "    return list(synonyms)\n",
    "\n",
    "def augment_text(text: str, replacement_prob: float = 0.1, skip_words: set = None) -> str:\n",
    "    \"\"\"\n",
    "    Perform data augmentation on the text:\n",
    "      - Replace each word with a synonym with a certain probability (replacement_prob)\n",
    "      - Protect domain keywords (e.g., FDA, Listeria, E. coli, etc.) from replacement\n",
    "    \"\"\"\n",
    "    if skip_words is None:\n",
    "        skip_words = {\"FDA\", \"Listeria\", \"E. coli\", \"Salmonella\", \"Maytag\", \"CDC\"}\n",
    "    \n",
    "    words = text.split()\n",
    "    augmented_words = []\n",
    "    for word in words:\n",
    "        # Only replace alphabetic words that are not in the protection list\n",
    "        if word.isalpha() and word not in skip_words and random.random() < replacement_prob:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                new_word = random.choice(synonyms)\n",
    "                augmented_words.append(new_word)\n",
    "            else:\n",
    "                augmented_words.append(word)\n",
    "        else:\n",
    "            augmented_words.append(word)\n",
    "    return \" \".join(augmented_words)\n",
    "\n",
    "def process_data(input_file: str,\n",
    "                 output_file: str,\n",
    "                 tokenizer_name: str = \"bert-base-uncased\",\n",
    "                 max_length: int = 512,\n",
    "                 augmentation_prob: float = 0.1):\n",
    "    \"\"\"\n",
    "    Overall data preprocessing and augmentation process:\n",
    "      1. Read CSV data (the file contains the fields: docid, text, Subtask1_Label)\n",
    "      2. Clean each article to generate cleaned_text\n",
    "      3. Generate segments (in list form) using manual segmentation based on max_length\n",
    "      4. Perform data augmentation on the cleaned text to generate augmented_text\n",
    "      5. Save the results into a new CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    segments_list = []\n",
    "    augmented_texts = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        original_text = str(row[\"text\"])\n",
    "        # 1. Clean the text\n",
    "        cleaned = clean_text(original_text)\n",
    "        # 2. Manual segmentation (hierarchical processing): obtain a list of segments with token count not exceeding max_length\n",
    "        segments = segment_text(cleaned, tokenizer, max_length)\n",
    "        # 3. Data augmentation (synonym replacement)\n",
    "        augmented = augment_text(cleaned, replacement_prob=augmentation_prob)\n",
    "        \n",
    "        cleaned_texts.append(cleaned)\n",
    "        segments_list.append(segments)\n",
    "        augmented_texts.append(augmented)\n",
    "    \n",
    "    df[\"cleaned_text\"] = cleaned_texts\n",
    "    df[\"segments\"] = segments_list\n",
    "    df[\"augmented_text\"] = augmented_texts\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"[INFO] Preprocessing complete. Output saved to: {output_file}\")\n",
    "\n",
    "# ===============================================\n",
    "# Run the following code in Jupyter Notebook\n",
    "# ===============================================\n",
    "input_file = \"SMM4H-2025-Task5-Validation_subtask1.csv\"  # Input file path\n",
    "output_file = \"preprocessed_SMM4H-2025-Task5-Validation_subtask1.csv\"  # Output file path\n",
    "tokenizer_name = \"bert-base-uncased\"  # Change the model name if needed\n",
    "max_seq_length = 512  # Maximum token count for Transformer models\n",
    "augmentation_probability = 0.1  # Probability for synonym replacement\n",
    "\n",
    "process_data(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    max_length=max_seq_length,\n",
    "    augmentation_prob=augmentation_probability\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
